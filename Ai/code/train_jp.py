#ì§ì—… íƒœê·¸ ì¶”ê°€í•œ ë²„ì „

import torch
import json
from datasets import load_dataset
from transformers import (
    AutoModelForTokenClassification,
    AutoTokenizer,
    DataCollatorForTokenClassification,
    TrainingArguments,
    Trainer,
)
import numpy as np
import evaluate

# --- 1. ì„¤ì • ë° ë¡œë”© ---
# ê²½ë¡œ ì„¤ì •
base_model_path = "./local_models/KoELECTRA-small-v3-modu-ner"  # 1ë‹¨ê³„ì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ì›ë³¸ ëª¨ë¸ ê²½ë¡œ
dataset_path = "./DATASET/data.jsonl" # 2ë‹¨ê³„ì—ì„œ ìƒì„±í•œ ë°ì´í„°ì…‹ ê²½ë¡œ
finetuned_model_output_path = "./local_models/finetuned_ner_model" # ìµœì¢… íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì´ ì €ì¥ë  ê²½ë¡œ

# ë¼ë²¨ ë§µ ë¡œë”© (2ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•œ ê²ƒê³¼ ë°˜ë“œì‹œ ë™ì¼í•´ì•¼ í•¨)
label2id = {
    "O": 0,
    "B-PS": 1, "I-PS": 2,          # ì‚¬ëŒ
    "B-LC": 3, "I-LC": 4,          # ìœ„ì¹˜
    "B-OG": 5, "I-OG": 6,          # ê¸°ê´€
    "B-DT": 7, "I-DT": 8,          # ë‚ ì§œ
    "B-BD": 9, "I-BD": 10,         # ìƒë…„ì›”ì¼
    "B-PN": 11, "I-PN": 12,        # ì „í™”ë²ˆí˜¸
    "B-SSN": 13, "I-SSN": 14,      # ì£¼ë¯¼ë²ˆí˜¸
    "B-AN": 15, "I-AN": 16,        # ê³„ì¢Œë²ˆí˜¸
    "B-CCD": 17, "I-CCD": 18,      # ì¹´ë“œë²ˆí˜¸
    "B-CVC": 19, "I-CVC": 20,      # CVC
    "B-EM": 21, "I-EM": 22,        # ì´ë©”ì¼
    "B-PPS": 23, "I-PPS": 24,      # ì—¬ê¶Œë²ˆí˜¸ (PPS: Passport)
    "B-AG": 25, "I-AG": 26,        # ë‚˜ì´
    "B-GD": 27, "I-GD": 28,        # ì„±ë³„
    "B-JOB":29, "I-JOB":30         # ì§ì—…
}

id2label = {v: k for k, v in label2id.items()}
all_labels = list(label2id.keys()) # ëª¨ë“  ë¼ë²¨ ë¦¬ìŠ¤íŠ¸

# í† í¬ë‚˜ì´ì € ë¡œë”©
tokenizer = AutoTokenizer.from_pretrained(base_model_path)

# ëª¨ë¸ ë¡œë”©
# **ì¤‘ìš”**: ignore_mismatched_sizes=Trueë¥¼ ì¶”ê°€í•˜ì—¬ í¬ê¸°ê°€ ë‹¤ë¥¸ ë¶„ë¥˜ ë ˆì´ì–´ëŠ” ë¬´ì‹œí•˜ê³  ìƒˆë¡œ ì´ˆê¸°í™”
model = AutoModelForTokenClassification.from_pretrained(
    base_model_path,
    num_labels=len(all_labels),
    id2label=id2label,
    label2id=label2id,
    ignore_mismatched_sizes=True,
)

# ë°ì´í„°ì…‹ ë¡œë”©
raw_datasets = load_dataset('json', data_files=dataset_path, split="train")

# --- 2. ë°ì´í„° ì „ì²˜ë¦¬ ---
def tokenize_and_align_labels(examples):
    """ë°ì´í„°ì…‹ì„ í† í¬ë‚˜ì´ì§•í•˜ê³ , ë¼ë²¨ì„ í† í°ì— ë§ê²Œ ì •ë ¬í•˜ëŠ” í•¨ìˆ˜"""
    tokenized_inputs = tokenizer(
        examples["tokens"], 
        truncation=True, 
        is_split_into_words=True # ì…ë ¥ì´ ì´ë¯¸ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ì„ì„ ëª…ì‹œ
    )

    labels = []
    for i, label in enumerate(examples[f"ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None: # [CLS], [SEP] ê°™ì€ ìŠ¤í˜ì…œ í† í°
                label_ids.append(-100)
            elif word_idx != previous_word_idx: # ìƒˆë¡œìš´ ë‹¨ì–´ì˜ ì²« í† í°
                label_ids.append(label[word_idx])
            else: # ê°™ì€ ë‹¨ì–´ì˜ ì„œë¸Œì›Œë“œ í† í°
                label_ids.append(-100)
            previous_word_idx = word_idx
        labels.append(label_ids)
    
    tokenized_inputs["labels"] = labels
    return tokenized_inputs


# ======================================================================
# â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ ë””ë²„ê¹… ë° ê²€ì¦ ë¡œì§ â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
# ======================================================================
print("INFO: ğŸ” ë°ì´í„°ì…‹ ê°œë³„ ê²€ì¦ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
found_error = False
for i, example in enumerate(raw_datasets):
    # 1. 'tokens' ë˜ëŠ” 'ner_tags' í‚¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if 'tokens' not in example or 'ner_tags' not in example:
        print(f"ERROR: ğŸ’¥ {i}ë²ˆì§¸ ë°ì´í„°ì— 'tokens' ë˜ëŠ” 'ner_tags' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. í™•ì¸í•´ì£¼ì„¸ìš”.")
        print(f"   - ë‚´ìš©: {example}")
        found_error = True
        break
        
    # 2. ê°œìˆ˜ ë¶ˆì¼ì¹˜ í™•ì¸
    if len(example['tokens']) != len(example['ner_tags']):
        print(f"ERROR: ğŸ’¥ {i}ë²ˆì§¸ ë°ì´í„°ì—ì„œ ê°œìˆ˜ ë¶ˆì¼ì¹˜ ë°œê²¬!")
        print(f"   - í† í° ({len(example['tokens'])}ê°œ): {example['tokens']}")
        print(f"   - ë¼ë²¨ ({len(example['ner_tags'])}ê°œ): {example['ner_tags']}")
        found_error = True
        break

    # 3. ë¹ˆ ë¬¸ìì—´("") ë˜ëŠ” None ê°’ í™•ì¸
    if "" in example['tokens'] or None in example['tokens']:
        print(f"ERROR: ğŸ’¥ {i}ë²ˆì§¸ ë°ì´í„°ì˜ 'tokens' ë¦¬ìŠ¤íŠ¸ì— ë¹ˆ ë¬¸ìì—´ì´ë‚˜ None ê°’ì´ ìˆìŠµë‹ˆë‹¤.")
        print(f"   - í† í°: {example['tokens']}")
        found_error = True
        break
        
    # 4. ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì§ì ‘ ì‹¤í–‰í•˜ì—¬ ì‹¤ì œ ì˜¤ë¥˜ ì¬í˜„
    try:
        # í•¨ìˆ˜ê°€ ë°°ì¹˜(batch) ë‹¨ìœ„ë¡œ ë™ì‘í•˜ë¯€ë¡œ, ê°œë³„ ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ì–´ í…ŒìŠ¤íŠ¸
        mock_batch = {
            'tokens': [example['tokens']],
            'ner_tags': [example['ner_tags']]
        }
        tokenize_and_align_labels(mock_batch)
    except IndexError as e:
        print(f"ERROR: ğŸ’¥ {i}ë²ˆì§¸ ë°ì´í„° ì²˜ë¦¬ ì¤‘ 'IndexError' ë°œìƒ!")
        print(f"   - ì´ ë°ì´í„°ì— ë¬¸ì œê°€ ìˆì„ í™•ë¥ ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤. ë‚´ìš©ì„ ìì„¸íˆ í™•ì¸í•´ì£¼ì„¸ìš”.")
        print(f"   - í† í° ({len(example['tokens'])}ê°œ): {example['tokens']}")
        print(f"   - ë¼ë²¨ ({len(example['ner_tags'])}ê°œ): {example['ner_tags']}")
        found_error = True
        break

if found_error:
    print("\nINFO: ğŸ›‘ ìœ„ì— ë³´ê³ ëœ ë¬¸ì œê°€ ìˆëŠ” ë°ì´í„°ë¥¼ 'data.jsonl' íŒŒì¼ì—ì„œ ìˆ˜ì •í•œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    exit() # í”„ë¡œê·¸ë¨ ì¢…ë£Œ
else:
    print("INFO: âœ… ëª¨ë“  ë°ì´í„°ê°€ ê°œë³„ ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤. ëª¨ë¸ í›ˆë ¨ì„ ê³„ì†í•©ë‹ˆë‹¤.")
# ======================================================================
# â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–² ê²€ì¦ ë¡œì§ ë â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
# ======================================================================


# ë°ì´í„°ì…‹ì— ì „ì²˜ë¦¬ í•¨ìˆ˜ ì ìš©
tokenized_datasets = raw_datasets.map(tokenize_and_align_labels, batched=True, remove_columns=raw_datasets.column_names)

# ë°ì´í„° ë¡œë”ë¥¼ ìœ„í•œ collator ì„¤ì •
data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)

# --- 3. í›ˆë ¨(Training) ì„¤ì • ---
# í‰ê°€ ì§€í‘œ ì„¤ì • (seqeval)
seqeval = evaluate.load("seqeval")

def compute_metrics(p):
    """ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì„ ë°›ì•„ ì„±ëŠ¥ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜"""
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    true_predictions = [
        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = seqeval.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }

# Trainerë¥¼ ìœ„í•œ ì¸ì(argument) ì„¤ì •
training_args = TrainingArguments(
    output_dir=finetuned_model_output_path,      # ëª¨ë¸ê³¼ ë¡œê·¸ê°€ ì €ì¥ë  ê²½ë¡œ
    learning_rate=2e-5,                          # í•™ìŠµë¥ 
    per_device_train_batch_size=8,               # ì¥ì¹˜(GPU)ë‹¹ í›ˆë ¨ ë°°ì¹˜ ì‚¬ì´ì¦ˆ
    num_train_epochs=20,                          # ì´ í›ˆë ¨ ì—í¬í¬ ìˆ˜
    weight_decay=0.01,                           # ê°€ì¤‘ì¹˜ ê°ì†Œ (ê³¼ì í•© ë°©ì§€)
    logging_strategy="epoch",                    # ì—í¬í¬ ë‹¨ìœ„ë¡œ ë¡œê·¸ ê¸°ë¡
    # evaluation_strategy="epoch",             # ì‹¤ì œë¡œëŠ” ê²€ì¦ ë°ì´í„°ì…‹ì„ ë¶„ë¦¬í•˜ì—¬ í‰ê°€í•´ì•¼ í•¨
    # save_strategy="epoch",
    # load_best_model_at_end=True,
)

# Trainer ê°ì²´ ìƒì„±
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets,
    # eval_dataset=... # ê²€ì¦ ë°ì´í„°ì…‹ì„ ì—¬ê¸°ì— ì „ë‹¬
    tokenizer=tokenizer,
    data_collator=data_collator,
    # compute_metrics=compute_metrics, # í‰ê°€ ì‹œ ì£¼ì„ í•´ì œ
)

# --- 4. í›ˆë ¨ ì‹¤í–‰ ë° ì €ì¥ ---
print("INFO: ğŸš€ ëª¨ë¸ íŒŒì¸íŠœë‹ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
trainer.train()

print("INFO: âœ… í›ˆë ¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
trainer.save_model(finetuned_model_output_path)
print(f"INFO: âœ… íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì´ '{finetuned_model_output_path}' ê²½ë¡œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")